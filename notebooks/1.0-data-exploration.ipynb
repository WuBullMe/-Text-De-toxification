{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae25b2a8-9f7f-4370-93bd-e88241c53710",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a9d95-6734-468d-b075-4430fe628a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528884ec-0f2f-458c-a5e4-33b419fcb681",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8b0c9-3bd7-4b5f-90a8-01d4792f48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read corpus of filename\n",
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        questions = line.split('\\t')\n",
    "        data.append(questions)\n",
    "        data[-1][-1] = data[-1][-1][:-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91444b-4194-4448-a80a-2dc07deebfcd",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78da850-59fc-4266-a8d7-bc19e99a9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_col = ['reference', 'translation']\n",
    "num_col = ['ref_tox', 'trn_tox', 'similarity', 'lenght_diff']\n",
    "\n",
    "data = read_corpus(\"data/raw/filtered.tsv\")\n",
    "data[0][0] = 'id'\n",
    "for i in range(1, len(data)):\n",
    "    if float(data[i][-2]) > float(data[i][-1]):\n",
    "        data[i][-2], data[i][-1] = data[i][-1], data[i][-2]\n",
    "        data[i][1], data[i][2] = data[i][2], data[i][1]\n",
    "        \n",
    "Data = pd.DataFrame(data[1:], columns=data[0])\n",
    "for num in num_col:\n",
    "    Data[num] = pd.to_numeric(Data[num])\n",
    "\n",
    "Data.index = pd.to_numeric(Data['id']).values\n",
    "Data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae02069-fb12-4d4b-b1f8-7e2a494ae867",
   "metadata": {},
   "source": [
    "# Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911e3a2-e2d3-493f-ba26-e17512411386",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<sos>\", 1: \"<eos>\", 2 : \"<pad>\"}\n",
    "        self.n_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751187a-d070-471d-9dea-2c1f32dd7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10 # Max length of sentences\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    return nltk.word_tokenize(s)\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(nltk.word_tokenize(p[0])) < MAX_LENGTH and \\\n",
    "        len(nltk.word_tokenize(p[1])) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filter(norm_ref, norm_trs):\n",
    "    filter_ref = []\n",
    "    filter_trs = []\n",
    "    for pair in zip(norm_ref, norm_trs):\n",
    "        if filterPair(pair):\n",
    "            filter_ref.append(pair[0])\n",
    "            filter_trs.append(pair[1])\n",
    "    return filter_ref, filter_trs\n",
    "\n",
    "\n",
    "def prepareData(data):\n",
    "    # Filter every data\n",
    "    filt_ref = [row for row in data['reference']]\n",
    "    filt_trs = [row for row in data['translation']]\n",
    "    \n",
    "    filt_ref, filt_trs = filter(norm_ref, norm_trs)\n",
    "    # Make Vocabulary instances\n",
    "    vocab_tox = Vocabulary('tox-vocab')\n",
    "    vocab_detox = Vocabulary('detox-vocab')\n",
    "    pairs = []\n",
    "    for row in zip(filt_ref, filt_trs):\n",
    "        pairs.append(row)\n",
    "\n",
    "    for row in filt_ref:\n",
    "        vocab_tox.addSentence(row)\n",
    "\n",
    "    for row in filt_trs:\n",
    "        vocab_detox.addSentence(row)\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(vocab_tox.name, vocab_tox.n_words)\n",
    "    print(vocab_detox.name, vocab_detox.n_words)\n",
    "\n",
    "    return vocab_tox, vocab_detox, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059971a-6d0d-4468-8117-10ff63b9c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(vocab_tox, pair[0])\n",
    "    target_tensor = tensorFromSentence(vocab_detox, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size, vocab_tox, vocab_detox, pairs, p=0.9):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH + 1), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH + 1), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(vocab_tox, inp)\n",
    "        tgt_ids = indexesFromSentence(vocab_detox, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        while len(inp_ids) < MAX_LENGTH + 1:\n",
    "            inp_ids.append(PAD_token)\n",
    "        \n",
    "        while len(tgt_ids) < MAX_LENGTH + 1:\n",
    "            tgt_ids.append(PAD_token)\n",
    "        \n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    idx = [i for i in range(n)]\n",
    "    train_idx, val_idx = train_test_split(idx, train_size=p, random_state=420)\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids[train_idx]).to(device),\n",
    "                               torch.LongTensor(target_ids[train_idx]).to(device))\n",
    "    val_data = TensorDataset(torch.LongTensor(input_ids[val_idx]).to(device),\n",
    "                               torch.LongTensor(target_ids[val_idx]).to(device))\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
