{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae25b2a8-9f7f-4370-93bd-e88241c53710",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf8a9d95-6734-468d-b075-4430fe628a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91444b-4194-4448-a80a-2dc07deebfcd",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c78da850-59fc-4266-a8d7-bc19e99a9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_col = ['reference', 'translation']\n",
    "num_col = ['ref_tox', 'trn_tox', 'similarity', 'lenght_diff']\n",
    "\n",
    "data = pd.read_csv(\"../data/raw/filtered.tsv\", sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f39d9ac9-5e33-4bcf-a688-a90acb612905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.065473</td>\n",
       "      <td>0.999039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.994215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.999348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>You didn't know that Estelle had stolen some f...</td>\n",
       "      <td>you didn't know that Estelle stole your fish f...</td>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.949143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>It'il suck the life out of you!</td>\n",
       "      <td>you'd be sucked out of your life!</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>I can't fuckin' take that, bruv.</td>\n",
       "      <td>I really can't take this.</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>They called me a fucking hero. The truth is I ...</td>\n",
       "      <td>they said I was a hero, but I didn't care.</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>I did not screw him.</td>\n",
       "      <td>I didn't fuck him.</td>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.994174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577777 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "0       If Alkar is flooding her with psychic waste, t...   \n",
       "1                               Now you're getting nasty.   \n",
       "2                Well, we could spare your life, for one.   \n",
       "3               Ah! Monkey, you've got to snap out of it.   \n",
       "4                        I've got orders to put her down.   \n",
       "...                                                   ...   \n",
       "577772  You didn't know that Estelle had stolen some f...   \n",
       "577773                    It'il suck the life out of you!   \n",
       "577774                   I can't fuckin' take that, bruv.   \n",
       "577775  They called me a fucking hero. The truth is I ...   \n",
       "577776                               I did not screw him.   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "0       if Alkar floods her with her mental waste, it ...    0.785171   \n",
       "1                             you're becoming disgusting.    0.749687   \n",
       "2                           well, we can spare your life.    0.919051   \n",
       "3                            monkey, you have to wake up.    0.664333   \n",
       "4                              I have orders to kill her.    0.726639   \n",
       "...                                                   ...         ...   \n",
       "577772  you didn't know that Estelle stole your fish f...    0.870322   \n",
       "577773                  you'd be sucked out of your life!    0.722897   \n",
       "577774                          I really can't take this.    0.617511   \n",
       "577775         they said I was a hero, but I didn't care.    0.679613   \n",
       "577776                                 I didn't fuck him.    0.868475   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "0          0.010309  0.014195  0.981983  \n",
       "1          0.071429  0.065473  0.999039  \n",
       "2          0.268293  0.213313  0.985068  \n",
       "3          0.309524  0.053362  0.994215  \n",
       "4          0.181818  0.009402  0.999348  \n",
       "...             ...       ...       ...  \n",
       "577772     0.030769  0.000121  0.949143  \n",
       "577773     0.058824  0.996124  0.215794  \n",
       "577774     0.212121  0.984538  0.000049  \n",
       "577775     0.358209  0.991945  0.000124  \n",
       "577776     0.095238  0.009480  0.994174  \n",
       "\n",
       "[577777 rows x 6 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda30f2-b16b-4986-ace0-24d50ac38a9b",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "    * Put more toxicity sentence into reference column\n",
    "    * Convert abbreviations into original form (I'm -> I am)\n",
    "    * Split sentece using word_tokenize\n",
    "    * Correct spelling of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ddd05693-c51e-4864-8331-37954e4411b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['1', '2', '3', '4', '5', '6']\n",
    "data['reference'] = np.where(data['5'] < data['6'], data['1'], data['2'])\n",
    "data['translation'] = np.where(data['5'] < data['6'], data['2'], data['1'])\n",
    "data['ref_tox'] = np.where(data['5'] < data['6'], data['5'], data['6'])\n",
    "data['trn_tox'] = np.where(data['5'] < data['6'], data['6'], data['5'])\n",
    "data['similarity'] = data['3']\n",
    "data['length_diff'] = data['4']\n",
    "data = data.drop(['1', '2', '3', '4', '5', '6'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "99418f5c-ea9e-4509-a2cd-d9e0332cc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577777/577777 [04:42<00:00, 2047.28it/s]\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker(distance=1)\n",
    "columns = data.columns\n",
    "\n",
    "Data = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    row = data.loc[i].values\n",
    "    for it in range(0, 2):\n",
    "        row[it] = contractions.fix(row[it].lower())\n",
    "        row[it] = nltk.word_tokenize(row[it])\n",
    "        words = []\n",
    "        for word in row[it]:\n",
    "            for sub_word in word.split('-'):\n",
    "                words.append(sub_word)\n",
    "        new_row = []\n",
    "        for word in words:\n",
    "            res = spell.correction(word)\n",
    "            if res is not None:\n",
    "                new_row.append(res)\n",
    "            else:\n",
    "                if len([ch for ch in string.punctuation if ch in word]) == 0:\n",
    "                    new_row.append(word)\n",
    "        row[it] = ' '.join(new_row)\n",
    "    Data.append(row)\n",
    "data = pd.DataFrame(Data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9b1306fc-d280-4fc2-b656-581eeb3667e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/interim/preprocessed_filtered_1.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93388cb6-8f66-4f66-86ed-232dce188293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "      <th>similarity</th>\n",
       "      <th>length_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if altar is flooding her with psychic waste , ...</td>\n",
       "      <td>if altar floods her with her mental waste , it...</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now you are getting nasty .</td>\n",
       "      <td>you are becoming disgusting .</td>\n",
       "      <td>0.065473</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well , we could spare your life , for one .</td>\n",
       "      <td>well , we can spare your life .</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ah ! monkey , you have got to snap out of it .</td>\n",
       "      <td>monkey , you have to wake up .</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have got orders to put her down .</td>\n",
       "      <td>i have orders to kill her .</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>you did not know that estelle had stolen some ...</td>\n",
       "      <td>you did not know that estelle stole your fish ...</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.949143</td>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>you would be sucked out of your life !</td>\n",
       "      <td>it'il suck the life out of you !</td>\n",
       "      <td>0.215794</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>i really can not take this .</td>\n",
       "      <td>i can not fuckin ' take that , bruv .</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>they said i was a hero , but i did not care .</td>\n",
       "      <td>they called me a fucking hero . the truth is i...</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>i did not screw him .</td>\n",
       "      <td>i did not fuck him .</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.994174</td>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577777 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "0       if altar is flooding her with psychic waste , ...   \n",
       "1                             now you are getting nasty .   \n",
       "2             well , we could spare your life , for one .   \n",
       "3          ah ! monkey , you have got to snap out of it .   \n",
       "4                     i have got orders to put her down .   \n",
       "...                                                   ...   \n",
       "577772  you did not know that estelle had stolen some ...   \n",
       "577773             you would be sucked out of your life !   \n",
       "577774                       i really can not take this .   \n",
       "577775      they said i was a hero , but i did not care .   \n",
       "577776                              i did not screw him .   \n",
       "\n",
       "                                              translation   ref_tox   trn_tox  \\\n",
       "0       if altar floods her with her mental waste , it...  0.014195  0.981983   \n",
       "1                           you are becoming disgusting .  0.065473  0.999039   \n",
       "2                         well , we can spare your life .  0.213313  0.985068   \n",
       "3                          monkey , you have to wake up .  0.053362  0.994215   \n",
       "4                             i have orders to kill her .  0.009402  0.999348   \n",
       "...                                                   ...       ...       ...   \n",
       "577772  you did not know that estelle stole your fish ...  0.000121  0.949143   \n",
       "577773                   it'il suck the life out of you !  0.215794  0.996124   \n",
       "577774              i can not fuckin ' take that , bruv .  0.000049  0.984538   \n",
       "577775  they called me a fucking hero . the truth is i...  0.000124  0.991945   \n",
       "577776                               i did not fuck him .  0.009480  0.994174   \n",
       "\n",
       "        similarity  length_diff  \n",
       "0         0.785171     0.010309  \n",
       "1         0.749687     0.071429  \n",
       "2         0.919051     0.268293  \n",
       "3         0.664333     0.309524  \n",
       "4         0.726639     0.181818  \n",
       "...            ...          ...  \n",
       "577772    0.870322     0.030769  \n",
       "577773    0.722897     0.058824  \n",
       "577774    0.617511     0.212121  \n",
       "577775    0.679613     0.358209  \n",
       "577776    0.868475     0.095238  \n",
       "\n",
       "[577777 rows x 6 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae02069-fb12-4d4b-b1f8-7e2a494ae867",
   "metadata": {},
   "source": [
    "# Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9911e3a2-e2d3-493f-ba26-e17512411386",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "        Vocabulary of words:\n",
    "            * Initially filled by 3 tokens\n",
    "                * <sos> -> start of sequence\n",
    "                * <eos> -> end of sequence\n",
    "                * <pad> -> fill to MAX_length\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<sos>\": 0, \"<eos>\": 1, \"<pad>\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<sos>\", 1: \"<eos>\", 2 : \"<pad>\"}\n",
    "        self.n_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6751187a-d070-471d-9dea-2c1f32dd7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 11 # Max length of sentences\n",
    "\n",
    "# Split sentence\n",
    "def getList(sentence):\n",
    "    return sentence.split(' ')\n",
    "\n",
    "# Filter pair(toxic text, or translated text) by number of words < MAX_LENGTH\n",
    "def filterPair(p):\n",
    "    return len(getList(p[0])) < MAX_LENGTH - 1 and \\\n",
    "        len(getList(p[1])) < MAX_LENGTH - 1 # EOS\n",
    "\n",
    "\n",
    "# Filter every pair(toxic text, or translated text) by number of words < MAX_LENGTH\n",
    "def filter(norm_ref, norm_trs):\n",
    "    filter_ref = []\n",
    "    filter_trs = []\n",
    "    for pair in zip(norm_ref, norm_trs):\n",
    "        if filterPair(pair):\n",
    "            filter_ref.append(pair[0])\n",
    "            filter_trs.append(pair[1])\n",
    "    return filter_ref, filter_trs\n",
    "\n",
    "\n",
    "# Create vocabulary for toxic text and translated, also pair of them\n",
    "def prepareData(data):\n",
    "    # Normalize every data and filter\n",
    "    norm_ref = [row for row in data['reference']]\n",
    "    norm_trs = [row for row in data['translation']]\n",
    "    \n",
    "    norm_ref, norm_trs = filter(norm_ref, norm_trs)\n",
    "    # Make Vocabulary instances\n",
    "    vocab_tox = Vocabulary('tox-vocab')\n",
    "    vocab_detox = Vocabulary('detox-vocab')\n",
    "    pairs = []\n",
    "    for row in zip(norm_ref, norm_trs):\n",
    "        pairs.append(row)\n",
    "\n",
    "    for row in norm_ref:\n",
    "        vocab_tox.addSentence(row)\n",
    "\n",
    "    for row in norm_trs:\n",
    "        vocab_detox.addSentence(row)\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(vocab_tox.name, vocab_tox.n_words)\n",
    "    print(vocab_detox.name, vocab_detox.n_words)\n",
    "\n",
    "    return vocab_tox, vocab_detox, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9059971a-6d0d-4468-8117-10ff63b9c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert every word in sentence to indexes of vocabulary\n",
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in getList(sentence)]\n",
    "\n",
    "# Convert every word in sentence to indexes of vocabulary in tensor format\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "# Convert every word in pair sentences to indexes of vocabulary in tensor format\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(vocab_tox, pair[0])\n",
    "    target_tensor = tensorFromSentence(vocab_detox, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size, vocab_tox, vocab_detox, pairs, train_size=0.9):\n",
    "    \"\"\"\n",
    "        Return dataloaders of data pairs by given parameters:\n",
    "            :param batch_size: dataloader of batch_size\n",
    "            :param vocab_tox: vocabulary for toxic text\n",
    "            :param vocab_detox: vocabulary for translated text\n",
    "            :param pairs: data to create dataloader\n",
    "            :param train_size: proportion for train part\n",
    "    \"\"\"\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(vocab_tox, inp)\n",
    "        tgt_ids = indexesFromSentence(vocab_detox, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        while len(inp_ids) < MAX_LENGTH:\n",
    "            inp_ids.append(PAD_token)\n",
    "        \n",
    "        while len(tgt_ids) < MAX_LENGTH:\n",
    "            tgt_ids.append(PAD_token)\n",
    "        \n",
    "        input_ids[idx] = inp_ids\n",
    "        target_ids[idx] = tgt_ids\n",
    "\n",
    "    idx = [i for i in range(n)]\n",
    "    train_idx, val_idx = train_test_split(idx, train_size=train_size, random_state=420)\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids[train_idx]).to(device),\n",
    "                               torch.LongTensor(target_ids[train_idx]).to(device))\n",
    "    val_data = TensorDataset(torch.LongTensor(input_ids[val_idx]).to(device),\n",
    "                               torch.LongTensor(target_ids[val_idx]).to(device))\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
